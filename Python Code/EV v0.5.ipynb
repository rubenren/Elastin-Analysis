{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b7052bf",
   "metadata": {},
   "source": [
    "# Objective: train and evaluate a model\n",
    "\n",
    "#### Model for trial is the DUCK-Net model\n",
    "\n",
    "https://github.com/usagisukisuki/adaptive_t-vmf_dice_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725e0163",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b44d362",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import albumentations as albu\n",
    "import cv2\n",
    "import my_lib\n",
    "import tensorflow as tf\n",
    "import gc\n",
    "import pickle\n",
    "from keras.callbacks import CSVLogger\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import jaccard_score, precision_score, recall_score, accuracy_score, f1_score\n",
    "from ModelArchitecture.DiceLoss import dice_metric_loss\n",
    "from ModelArchitecture import DUCK_Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2d87e66",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4752c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First things first, set up our parameters\n",
    "\n",
    "img_size = 128\n",
    "learning_rate = 1e-4 # Controls how fast the loss goes down\n",
    "filters = 17 # Number of filters\n",
    "seed_value = 37\n",
    "batch_size = 8 # parallel computes\n",
    "optimizer = tf.keras.optimizers.RMSprop(learning_rate=learning_rate) # the optimizer to use\n",
    "\n",
    "ct = datetime.now()\n",
    "\n",
    "model_type = \"DuckNet\"\n",
    "\n",
    "progress_path = 'ProgressFull/' + 'progress_csv_' + model_type + '_filters_' + str(filters) +  '_' + str(ct).split('.')[0].replace(':','-') + '.csv'\n",
    "progressfull_path = 'ProgressFull/' + 'progress_' + model_type + '_filters_' + str(filters) + '_' + str(ct).split('.')[0].replace(':','-') + '.txt'\n",
    "plot_path = 'ProgressFull/' + 'progress_plot_' + model_type + '_filters_' + str(filters) + '_' + str(ct).split('.')[0].replace(':','-') + '.png'\n",
    "model_path = 'ModelSaveTensorFlow/' + model_type + '_filters_' + str(filters) + '_' + str(ct).split('.')[0].replace(':','-')\n",
    "\n",
    "EPOCHS = 10\n",
    "min_loss_for_saving = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f0cc2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "data_root = '../Example Images/'\n",
    "data_paths = [data_root + x for x in os.listdir(data_root) if x.find('.png') > 0]\n",
    "mask_root = '../data/Ground Truth Elastin/'\n",
    "mask_paths = [mask_root + x for x in os.listdir(mask_root) if x.find('.png') > 0]\n",
    "\n",
    "X = []\n",
    "Y = []\n",
    "for x, y in zip(data_paths, mask_paths):\n",
    "    temp_dict = my_lib.align_datum(x, y, size=img_size)\n",
    "    X += temp_dict['data']\n",
    "    Y += temp_dict['mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6505f97",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17600 17600\n"
     ]
    }
   ],
   "source": [
    "print(len(X), len(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37e9bb48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 128)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18f46813",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(X)\n",
    "Y = np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "009687b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_valid, y_train, y_valid = train_test_split(X, Y, test_size=0.2, shuffle=True, random_state = seed_value)\n",
    "x_valid, x_test, y_valid, y_test = train_test_split(x_valid, y_valid, test_size=0.5, shuffle=True, random_state=seed_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18d84fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the augmentations\n",
    "\n",
    "aug_train = albu.Compose([\n",
    "    albu.HorizontalFlip(),\n",
    "    albu.VerticalFlip(),\n",
    "    albu.Affine(scale=(0.5, 1.5), translate_percent=(-.125,.125), rotate=(-180, 180), shear=(-22.5, 22), always_apply=True)\n",
    "])\n",
    "\n",
    "def augment_images():\n",
    "    x_train_out = []\n",
    "    y_train_out = []\n",
    "\n",
    "    for i in range (len(x_train)):\n",
    "        ug = aug_train(image=x_train[i], mask=y_train[i])\n",
    "        x_train_out.append(ug['image'])  \n",
    "        y_train_out.append(ug['mask'])\n",
    "\n",
    "    return np.array(x_train_out), np.array(y_train_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e3fa45a1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting DUCK-Net\n"
     ]
    }
   ],
   "source": [
    "model = DUCK_Net.create_model(img_height=img_size, img_width=img_size, input_chanels=1, out_classes=1, starting_filters=filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9f047c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=optimizer, loss=dice_metric_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e2279f3f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training, epoch 0\n",
      "Learning Rate: 0.0002\n",
      "1760/1760 [==============================] - 991s 533ms/step - loss: 0.9924 - val_loss: 0.9877\n",
      "Loss Validation: 0.98752075\n",
      "Loss Test: 0.9884239\n",
      "Training, epoch 1\n",
      "Learning Rate: 0.0002\n",
      "1760/1760 [==============================] - 897s 510ms/step - loss: 0.9853 - val_loss: 0.9739\n",
      "Loss Validation: 0.9702134\n",
      "Loss Test: 0.9727487\n",
      "Training, epoch 2\n",
      "Learning Rate: 0.0002\n",
      "1760/1760 [==============================] - 893s 507ms/step - loss: 0.9698 - val_loss: 0.9639\n",
      "Loss Validation: 0.9543506\n",
      "Loss Test: 0.9592987\n",
      "Training, epoch 3\n",
      "Learning Rate: 0.0002\n",
      "1760/1760 [==============================] - 889s 505ms/step - loss: 0.9432 - val_loss: 0.9392\n",
      "Loss Validation: 0.8857551\n",
      "Loss Test: 0.90172786\n",
      "Training, epoch 4\n",
      "Learning Rate: 0.0002\n",
      "1760/1760 [==============================] - 895s 508ms/step - loss: 0.9085 - val_loss: 0.9246\n",
      "Loss Validation: 0.89384216\n",
      "Loss Test: 0.9078761\n",
      "Training, epoch 5\n",
      "Learning Rate: 0.0002\n",
      "1760/1760 [==============================] - 897s 509ms/step - loss: 0.8916 - val_loss: 0.9234\n",
      "Loss Validation: 0.88177377\n",
      "Loss Test: 0.89928955\n",
      "Training, epoch 6\n",
      "Learning Rate: 0.0002\n",
      "1760/1760 [==============================] - 889s 505ms/step - loss: 0.8882 - val_loss: 0.9063\n",
      "Loss Validation: 0.8019716\n",
      "Loss Test: 0.8350091\n",
      "Training, epoch 7\n",
      "Learning Rate: 0.0002\n",
      "1760/1760 [==============================] - 897s 510ms/step - loss: 0.8874 - val_loss: 0.9124\n",
      "Loss Validation: 0.86597335\n",
      "Loss Test: 0.880801\n",
      "Training, epoch 8\n",
      "Learning Rate: 0.0002\n",
      "1760/1760 [==============================] - 891s 506ms/step - loss: 0.8892 - val_loss: 0.9105\n",
      "Loss Validation: 0.7989808\n",
      "Loss Test: 0.8363226\n",
      "Training, epoch 9\n",
      "Learning Rate: 0.0002\n",
      "1760/1760 [==============================] - 931s 529ms/step - loss: 0.8828 - val_loss: 0.9121\n",
      "Loss Validation: 0.8797815\n",
      "Loss Test: 0.89540493\n"
     ]
    }
   ],
   "source": [
    "# Training the model\n",
    "\n",
    "step = 0\n",
    "\n",
    "for epoch in range(0, EPOCHS):\n",
    "    \n",
    "    print(f'Training, epoch {epoch}')\n",
    "    print('Learning Rate: ' + str(learning_rate))\n",
    "\n",
    "    step += 1\n",
    "        \n",
    "    image_augmented, mask_augmented = augment_images()\n",
    "    \n",
    "    csv_logger = CSVLogger(progress_path, append=True, separator=';')\n",
    "    \n",
    "    model.fit(x=image_augmented, y=mask_augmented, epochs=1, batch_size=batch_size,\\\n",
    "              validation_data=(x_valid, y_valid), verbose=1, callbacks=[csv_logger])\n",
    "    \n",
    "    prediction_valid = model.predict(x_valid, verbose=0)\n",
    "    loss_valid = dice_metric_loss(y_valid, prediction_valid)\n",
    "    \n",
    "    loss_valid = loss_valid.numpy()\n",
    "    print(\"Loss Validation: \" + str(loss_valid))\n",
    "        \n",
    "    prediction_test = model.predict(x_test, verbose=0)\n",
    "    loss_test = dice_metric_loss(y_test, prediction_test)\n",
    "    loss_test = loss_test.numpy()\n",
    "    print(\"Loss Test: \" + str(loss_test))\n",
    "        \n",
    "    with open(progressfull_path, 'a') as f:\n",
    "        f.write('epoch: ' + str(epoch) + '\\nval_loss: ' + str(loss_valid) + '\\ntest_loss: ' + str(loss_test) + '\\n\\n\\n')\n",
    "    \n",
    "    if min_loss_for_saving > loss_valid:\n",
    "        min_loss_for_saving = loss_valid\n",
    "        print(\"Saved model with val_loss: \", loss_valid)\n",
    "        model.save(model_path)\n",
    "        \n",
    "    del image_augmented\n",
    "    del mask_augmented\n",
    "\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "65a07d9d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3520/3520 [==============================] - 261s 73ms/step\n",
      "440/440 [==============================] - 33s 75ms/step\n",
      "440/440 [==============================] - 32s 74ms/step\n",
      "Predictions done\n",
      "Dice finished\n",
      "Miou finished\n",
      "Precision finished\n",
      "Recall finished\n",
      "Accuracy finished\n",
      "results_DuckNet_17.txt\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'dataset_type' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 68\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28mprint\u001b[39m(final_file)\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(final_file, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m---> 68\u001b[0m     f\u001b[38;5;241m.\u001b[39mwrite(\u001b[43mdataset_type\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     69\u001b[0m     f\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdice_train: \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(dice_train) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m dice_valid: \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(dice_valid) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m dice_test: \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(dice_test) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     70\u001b[0m     f\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmiou_train: \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(miou_train) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m miou_valid: \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(miou_valid) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m miou_test: \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(miou_test) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dataset_type' is not defined"
     ]
    }
   ],
   "source": [
    "# Computing the metrics and saving the results\n",
    "\n",
    "#print(\"Loading the model\")\n",
    "\n",
    "#model = tf.keras.models.load_model(model_path, custom_objects={'dice_metric_loss':dice_metric_loss})\n",
    "\n",
    "prediction_train = model.predict(x_train, batch_size=4)\n",
    "prediction_valid = model.predict(x_valid, batch_size=4)\n",
    "prediction_test = model.predict(x_test, batch_size=4)\n",
    "\n",
    "print(\"Predictions done\")\n",
    "\n",
    "dice_train = f1_score(np.ndarray.flatten(np.array(y_train, dtype=bool)),\n",
    "                           np.ndarray.flatten(prediction_train > 0.5))\n",
    "dice_test = f1_score(np.ndarray.flatten(np.array(y_test, dtype=bool)),\n",
    "                          np.ndarray.flatten(prediction_test > 0.5))\n",
    "dice_valid = f1_score(np.ndarray.flatten(np.array(y_valid, dtype=bool)),\n",
    "                           np.ndarray.flatten(prediction_valid > 0.5))\n",
    "\n",
    "print(\"Dice finished\")\n",
    "\n",
    "\n",
    "miou_train = jaccard_score(np.ndarray.flatten(np.array(y_train, dtype=bool)),\n",
    "                           np.ndarray.flatten(prediction_train > 0.5))\n",
    "miou_test = jaccard_score(np.ndarray.flatten(np.array(y_test, dtype=bool)),\n",
    "                          np.ndarray.flatten(prediction_test > 0.5))\n",
    "miou_valid = jaccard_score(np.ndarray.flatten(np.array(y_valid, dtype=bool)),\n",
    "                           np.ndarray.flatten(prediction_valid > 0.5))\n",
    "\n",
    "print(\"Miou finished\")\n",
    "\n",
    "\n",
    "precision_train = precision_score(np.ndarray.flatten(np.array(y_train, dtype=bool)),\n",
    "                                  np.ndarray.flatten(prediction_train > 0.5))\n",
    "precision_test = precision_score(np.ndarray.flatten(np.array(y_test, dtype=bool)),\n",
    "                                 np.ndarray.flatten(prediction_test > 0.5))\n",
    "precision_valid = precision_score(np.ndarray.flatten(np.array(y_valid, dtype=bool)),\n",
    "                                  np.ndarray.flatten(prediction_valid > 0.5))\n",
    "\n",
    "print(\"Precision finished\")\n",
    "\n",
    "\n",
    "recall_train = recall_score(np.ndarray.flatten(np.array(y_train, dtype=bool)),\n",
    "                            np.ndarray.flatten(prediction_train > 0.5))\n",
    "recall_test = recall_score(np.ndarray.flatten(np.array(y_test, dtype=bool)),\n",
    "                           np.ndarray.flatten(prediction_test > 0.5))\n",
    "recall_valid = recall_score(np.ndarray.flatten(np.array(y_valid, dtype=bool)),\n",
    "                            np.ndarray.flatten(prediction_valid > 0.5))\n",
    "\n",
    "print(\"Recall finished\")\n",
    "\n",
    "\n",
    "accuracy_train = accuracy_score(np.ndarray.flatten(np.array(y_train, dtype=bool)),\n",
    "                                np.ndarray.flatten(prediction_train > 0.5))\n",
    "accuracy_test = accuracy_score(np.ndarray.flatten(np.array(y_test, dtype=bool)),\n",
    "                               np.ndarray.flatten(prediction_test > 0.5))\n",
    "accuracy_valid = accuracy_score(np.ndarray.flatten(np.array(y_valid, dtype=bool)),\n",
    "                                np.ndarray.flatten(prediction_valid > 0.5))\n",
    "\n",
    "\n",
    "print(\"Accuracy finished\")\n",
    "\n",
    "\n",
    "final_file = 'results_' + model_type + '_' + str(filters) + '.txt'\n",
    "print(final_file)\n",
    "\n",
    "with open(final_file, 'a') as f:\n",
    "    f.write('dice_train: ' + str(dice_train) + ' dice_valid: ' + str(dice_valid) + ' dice_test: ' + str(dice_test) + '\\n\\n')\n",
    "    f.write('miou_train: ' + str(miou_train) + ' miou_valid: ' + str(miou_valid) + ' miou_test: ' + str(miou_test) + '\\n\\n')\n",
    "    f.write('precision_train: ' + str(precision_train) + ' precision_valid: ' + str(precision_valid) + ' precision_test: ' + str(precision_test) + '\\n\\n')\n",
    "    f.write('recall_train: ' + str(recall_train) + ' recall_valid: ' + str(recall_valid) + ' recall_test: ' + str(recall_test) + '\\n\\n')\n",
    "    f.write('accuracy_train: ' + str(accuracy_train) + ' accuracy_valid: ' + str(accuracy_valid) + ' accuracy_test: ' + str(accuracy_test) + '\\n\\n\\n\\n')\n",
    "\n",
    "print('File done')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
